/*!
  \addtogroup bos-spec BOS Server Interface
  @{

	\page title AFS-3 Programmer's Reference: BOS Server Interface 

\author Edward R. Zayas 
Transarc Corporation 
\version 1.0 
\date 28 August 1991 11:58 Copyright 1991 Transarc Corporation All Rights
Reserved FS-00-D161 

	\page chap1 Chapter 1: Overview 

	\section sec1-1 Section 1.1: Introduction 

\par
One of the important duties of an AFS system administrator is to insure that
processes on file server machines are properly installed and kept running. The
BOS Server was written as a tool for assisting administrators in these tasks.
An instance of the BOS Server runs on each AFS server machine, and has the
following specific areas of responsibility: 
\li Definition of the set of processes that are to be run on the machine on
which a given BOS Server executes. This definition may be changed dynamically
by system administrators. Programs may be marked as continuously or
periodically runnable. 
\li Automatic startup and restart of these specified processes upon server
bootup and program failure. The BOS Server also responds to administrator
requests for stopping and starting one or more of these processes. In addition,
the BOS Server is capable of restarting itself on command. 
\li Collection of information regarding the current status, command line
parameters, execution history, and log files generated by the set of server
programs. 
\li Management of the security information resident on the machine on which the
BOS Server executes. Such information includes the list of administratively
privileged people associated with the machine and the set of AFS File Server
encryption keys used in the course of file service. 
\li Management of the cell configuration information for the server machine in
question. This includes the name of the cell in which the server resides, along
with the list and locations of the servers within the cell providing AFS
database services (e.g., volume location, authentication, protection).
Installation of server binaries on the given machine. The BOS Server allows
several "generations" of server software to be kept on its machine.
Installation of new software for one or more server agents is handled by the
BOS Server, as is "rolling back" to a previous version should it prove more
stable than the currently-installed image. 
\par
Execution of commands on the server machine. An administrator may execute
arbitrary unix commands on a machine running the BOS Server. 
\par
Unlike many other AFS server processes, the BOS Server does not maintain a
cell-wide, replicated database. It does, however, maintain several databases
used exclusively on every machine on which it runs. 

	\section sec1-2 Section 1.2: Scope 

\par
This paper describes the design and structure of the AFS-3 BOS Server. The
scope of this work is to provide readers with a sufficiently detailed
description of the BOS Server so that they may construct client applications
that call the server's RPC interface routines. 

	\section sec1-3 Section 1.3: Document Layout 

\par
The second chapter discusses various aspects of the BOS Server's architecture.
First, one of the basic concepts is examined, namely the bnode. Providing the
complete description of a program or set of programs to be run on the given
server machine, a bnode is the generic definitional unit for the BOS Server's
duties. After bnodes have been explained, the set of standard directories on
which the BOS Server depends is considered. Also, the set of well-known files
within these directories is explored. Their uses and internal formats are
presented. After these sections, a discussion of BOS Server restart times
follows. The BOS Server has special support for two commonly-used restart
occasions, as described by this section. Finally, the organization and behavior
of the bosserver program itself is presented. 
\par
The third and final chapter provides a detailed examination of the
programmer-visible BOS Server constants and structures, along with a full
specification of the API for the RPC-based BOS Server functionality. 

	\section sec1-4 Section 1.4: Related Documents 

\par
This document is a member of a documentation suite providing programmer-level
specifications for the operation of the various AFS servers and agents, and the
interfaces they export, as well as the underlying RPC system they use to
communicate. The full suite of related AFS specification documents is listed
below: 
\li AFS-3 Programmer's Reference: Architectural Overview: This paper provides
an architectual overview of the AFS distributed file system, describing the
full set of servers and agents in a coherent way, illustrating their
relationships to each other and examining their interactions. 
\li AFS-3 Programmer's Reference: File Server/Cache Manager Interface: This
document describes the File Server and Cache Manager agents, which provide the
backbone file managment services for AFS. The collection of File Servers for a
cell supply centralized file storage for that site, and allow clients running
the Cache Manager component to acces those files in a high-performance, secure
fashion. 
\li AFS-3 Programmer's Reference:Volume Server/Volume Location Server
Interface: This document describes the services through which "containers" of
related user data are located and managed. 
\li AFS-3 Programmer's Reference: Protection Server Interface: This paper
describes the server responsible for mapping printable user names to and from
their internal AFS identifiers. The Protection Server also allows users to
create, destroy, and manipulate "groups" of users, which are suitable for
placement on ACLs. 
\li AFS-3 Programmer's Reference: Specification for the Rx Remote Procedure
Call Facility: This document specifies the design and operation of the remote
procedure call and lightweight process packages used by AFS. 
\par
In addition to these papers, the AFS 3.1 product is delivered with its own
user, administrator, installation, and command reference documents. 

	\page chap2 Chapter 2: BOS Server Architecture 

\par
This chapter considers some of the architectual features of the AFS-3 BOS
Server. First, the basic organizational and functional entity employed by the
BOS Server, the bnode, is discussed. Next, the set of files with which the
server interacts is examined. The notion of restart times is then explained in
detail. Finally, the organization and components of the bosserver program
itself, which implements the BOS Server, are presented. 

	\section sec2-1 Section 2.1: Bnodes 

	\subsection sec2-1-1 Section 2.1.1: Overview 

\par
The information required to manage each AFS-related program running on a File
Server machine is encapsulated in a bnode object. These bnodes serve as the
basic building blocks for BOS Server services. Bnodes have two forms of
existence: 
\li On-disk: The BosConfig file (see Section 2.3.4 below) defines the set of
bnodes for which the BOS Server running on that machine will be responsible,
along with specifying other information such as values for the two restart
times. This file provides permanent storage (i.e., between bootups) for the
desired list of programs for that server platform. 
\li In-memory: The contents of the BosConfig file are parsed and internalized
by the BOS Server when it starts execution. The basic data for a particular
server program is placed into a struct bnode structure. 
\par
The initial contents of the BosConfig file are typically set up during system
installation. The BOS Server can be directed, via its RPC interface, to alter
existing bnode entries in the BosConfig file, add new ones, and delete old
ones. Typically, this file is never edited directly. 

	\subsection sec2-1-2 Section 2.1.2: Bnode Classes 

\par
The descriptions of the members of the AFS server suite fall into three broad
classes of programs: 
\li Simple programs: This server class is populated by programs that simply
need to be kept running, and do not depend on other programs for correctness or
effectiveness. Examples of AFS servers falling into this category are the
Volume Location Server, Authentication Server, and Protection Server. Since its
members exhibit such straightforward behavior, this class of programs is
referred to as the simple class. 
\li Interrelated programs: The File Server program depends on two other
programs, and requires that they be executed at the appropriate times and in
the appropriate sequence, for correct operation. The first of these programs is
the Volume Server, which must be run concurrently with the File Server. The
second is the salvager, which repairs the AFS volume metadata on the server
partitions should the metadata become damaged. The salvager must not be run at
the same time as the File Server. In honor of the File Server trio that
inspired it, the class of programs consisting of groups of interrelated
processes is named the fs class. 
\li Periodic programs: Some AFS servers, such as the BackupServer, only need to
run every so often, but on a regular and well-defined basis. The name for this
class is taken from the unix tool that is typically used to define such regular
executions, namely the cron class. 
\par
The recognition and definition of these three server classes is exploited by
the BOS Server. Since all of the programs in a given class share certain common
characteristics, they may all utilize the same basic data structures to record
and manage their special requirements. Thus, it is not necessary to reimplement
all the operations required to satisfy the capabilities promised by the BOS
Server RPC interface for each and every program the BOS Server manages.
Implementing one set of operations for each server class is sufficient to
handle any and all server binaries to be run on the platform. 

	\subsection sec2-1-3 Section 2.1.3: Per-Class Bnode Operations 

\par
As mentioned above, only one set of basic routines must be implemented for each
AFS server class. Much like Sun's VFS/vnode interface [8], providing a common
set of routines for interacting with a given file system, regardless of its
underlying implementation and semantics, the BOS Server defines a common vector
of operations for a class of programs to be run under the BOS Server's
tutelage. In fact, it was this standardized file system interface that inspired
the "bnode" name. 
\par
The BOS Server manipulates the process or processes that are described by each
bnode by invoking the proper functions in the appropriate order from the
operation vector for that server class. Thus, the BOS Server itself operates in
a class-independent fashion. This allows each class to take care of the special
circumstances associated with it, yet to have the BOS Server itself be totally
unaware of what special actions (if any) are needed for the class. This
abstraction also allows more server classes to be implemented without any
significant change to the BOS Server code itself. 
\par
There are ten entries in this standardized class function array. The purpose
and usage of each individual class function is described in detail in Section
3.3.5. Much like the VFS system, a collection of macros is also provided in
order to simplify the invocation of these functions. These macros are described
in Section 3.5. The ten function slots are named here for convenience: 
\li create() 
\li timeout() 
\li getstat() 
\li setstat() 
\li delete() 
\li procexit() 
\li getstring() 
\li getparm() 
\li restartp() 
\li hascore() 

	\section sec2-2 Section 2.2: BOS Server Directories 

\par
The BOS Server expects the existence of the following directories on the local
disk of the platform on which it runs. These directories define where the
system binaries, log files, ubik databases, and other files lie. 
\li /usr/afs/bin: This directory houses the full set of AFS server binaries.
Such executables as bosserver, fileserver, vlserver, volserver, kaserver, and
ptserver reside here. 
\li /usr/afs/db: This directory serves as the well-known location on the
server's local disk for the ubik database replicas for this machine.
Specifically, the Authentication Server, Protection Server, and the Volume
Location Server maintain their local database images here. 
\li /usr/afs/etc: This directory hosts the files containing the security, cell,
and authorized system administrator list for the given machine. Specifically,
the CellServDB, KeyFile, License, ThisCell, and UserList files are stored here. 
\li /usr/afs/local: This directory houses the BosConfig file, which supplies
the BOS Server with the permanent set of bnodes to support. Also contained in
this directory are files used exclusively by the salvager. 
\li /usr/afs/logs: All of the AFS server programs that maintain log files
deposit them in this directory. 

	\section sec2-3 Section 2.3: BOS Server Files 

\par
Several files, some mentioned above, are maintained on the server's local disk
and manipulated by the BOS Server. This section examines many of these files,
and describes their formats. 

	\subsection sec2-3-1 Section 2.3.1: /usr/afs/etc/UserList 

\par
This file contains the names of individuals who are allowed to issue
"restricted" BOS Server commands (e.g., creating & deleting bnodes, setting
cell information, etc.) on the given hardware platform. The format is
straightforward, with one administrator name per line. If a cell grants joe and
schmoe these rights on a machine, that particular UserList will have the
following two lines: 
\n joe 
\n schmoe 

	\subsection sec2-3-2 Section 2.3.2: /usr/afs/etc/CellServDB 

\par
This file identifies the name of the cell to which the given server machine
belongs, along with the set of machines on which its ubik database servers are
running. Unlike the CellServDB found in /usr/vice/etc on AFS client machines,
this file contains only the entry for the home cell. It shares the formatting
rules with the /usr/vice/etc version of CellServDB. The contents of the
CellServDB file used by the BOS Server on host grand.central.org are: 
\code
>grand.central.org	#DARPA clearinghouse cell
192.54.226.100		#grand.central.org
192.54.226.101		#penn.central.org
\endcode

	\subsection sec2-3-3 Section 2.3.3: /usr/afs/etc/ThisCell 

\par
The BOS Server obtains its notion of cell membership from the ThisCell file in
the specified directory. As with the version of ThisCell found in /usr/vice/etc
on AFS client machines, this file simply contains the character string
identifying the home cell name. For any server machine in the grand.central.org
cell, this file contains the following: 
\code 
grand.central.org 
\endcode

	\subsection sec2-3-4 Section 2.3.4: /usr/afs/local/BosConfig 

\par
The BosConfig file is the on-disk representation of the collection of bnodes
this particular BOS Server manages. The BOS Server reads and writes to this
file in the normal course of its affairs. The BOS Server itself, in fact,
should be the only agent that modifies this file. Any changes to BosConfig
should be carried out by issuing the proper RPCs to the BOS Server running on
the desired machine. 
\par
The following is the text of the BosConfig file on grand.central.org. A
discussion of the contents follows immediately afterwards. 
\code
restarttime 11 0 4 0 0 checkbintime 3 0 5 0 0 
bnode simple kaserver 1 parm /usr/afs/bin/kaserver 
end bnode simple ptserver 1 parm 
/usr/afs/bin/ptserver end bnode simple vlserver 1 
parm /usr/afs/bin/vlserver end bnode fs fs 1 parm 
/usr/afs/bin/fileserver parm /usr/afs/bin/volserver 
parm /usr/afs/bin/salvager end bnode simple runntp 
1 parm /usr/afs/bin/runntp -localclock transarc.com 
end bnode simple upserver 1 parm 
/usr/afs/bin/upserver end bnode simple 
budb_server 1 parm /usr/afs/bin/budb_server end 
bnode cron backup 1 parm 
/usr/afs/backup/clones/lib/backup.csh daily parm 
05:00 end 
\endcode

\par
The first two lines of this file set the system and new-binary restart times
(see Section 2.4, below). They are optional, with the default system restart
time being every Sunday at 4:00am and the new-binary restart time being 5:00am
daily. Following the reserved words restarttime and checkbintime are five
integers, providing the mask, day, hour, minute, and second values (in decimal)
for the restart time, as diagrammed below: 
\code
restarttime <mask> <day> <hour> <minute> 
<second> checkbintime <mask> <day> <hour> 
<minute> <second> 
\endcode

\par
The range of acceptable values for these fields is presented in Section 3.3.1.
In this example, the restart line specifies a (decimal) mask value of 11,
selecting the KTIME HOUR, KTIME MIN, and KTIME DAY bits. This indicates that
the hour, minute, and day values are the ones to be used when matching times.
Thus, this line requests that system restarts occur on day 0 (Sunday), hour 4
(4:00am), and minute 0 within that hour. 
\par
The sets of lines that follow define the individual bnodes for the particular
machine. The first line of the bnode definition set must begin with the
reserved word bnode, followed by the type name, the instance name, and the
initial bnode goal: 
\code
bnode <type_name> <instance_name> <goal_val> 
\endcode

\par
The <type name> and <instance name> fields are both character strings, and the
<goal val> field is an integer. Acceptable values for the <type name> are
simple, fs, and cron. Acceptable values for <goal val> are defined in Section
3.2.3, and are normally restricted to the integer values representing BSTAT
NORMAL and BSTAT SHUTDOWN. Thus, in the bnode line defining the Authentication
Server, it is declared to be of type simple, have instance name kaserver, and
have 1 (BSTAT NORMAL) as a goal (e.g., it should be brought up and kept
running). 
\par
Following the bnode line in the BosConfig file may be one or more parm lines.
These entries represent the command line parameters that will be used to invoke
the proper related program or programs. The entire text of the line after the
parm reserved word up to the terminating newline is stored as the command line
string. 
\code
parm <command_line_text> 
\endcode

\par
After the parm lines, if any, the reserved word end must appear alone on a
line, marking the end of an individual bnode definition. 

	\subsection sec2-3-5 Section 2.3.5: /usr/afs/local/NoAuth 

\par
The appearance of this file is used to mark whether the BOS Server is to insist
on properly authenticated connections for its restricted operations or whether
it will allow any caller to exercise these commands. Not only is the BOS Server
affected by the presence of this file, but so are all of the bnodes objects the
BOS Server starts up. If /usr/afs/local/NoAuth is present, the BOS Server will
start all of its bnodes with the -noauth flag. 
\par
Completely unauthenticated AFS operation will result if this file is present
when the BOS Server starts execution. The file itself is typically empty. If
any data is put into the NoAuth file, it will be ignored by the system. 

	\subsection sec2-3-6 Section 2.3.6: /usr/afs/etc/KeyFile 

\par
This file stores the set of AFS encryption keys used for file service
operations. The file follows the format defined by struct afsconf key and
struct afsconf keys in include file afs/keys.h. For the reader's convenience,
these structures are detailed below: 
\code
#define AFSCONF_MAXKEYS 8 
struct afsconf_key { 
	long kvno; 
	char key[8]; 
}; 
struct afsconf_keys { 
	long nkeys; 
	struct afsconf_key key[AFSCONF_MAXKEYS]; 
}; 
\endcode
\par
The first longword of the file reveals the number of keys that may be found
there, with a maximum of AFSCONF MAXKEYS (8). The keys themselves follow, each
preceded by its key version number. 
\par
All information in this file is stored in network byte order. Each BOS Server
converts the data to the appropriate host byte order befor storing and
manipulating it. 

	\section sec2-4 Section 2.4: Restart Times 

\par
It is possible to manually start or restart any server defined within the set
of BOS Server bnodes from any AFS client machine, simply by making the
appropriate call to the RPC interface while authenticated as a valid
administrator (i.e., a principal listed in the UserList file on the given
machine). However, two restart situations merit the implementation of special
functionality within the BOS Server. There are two common occasions, occuring
on a regular basis, where the entire system or individual server programs
should be brought down and restarted: 
\par 
\b Complete \b system \b restart: To guard against the reliability and
performance problems caused by any core leaks in long-running programs, the
entire AFS system should be occasionally shut down and restarted periodically.
This action 'clears out' the memory system, and may result in smaller memory
images for these servers, as internal data structures are reinitialized back to
their starting sizes. It also allows AFS partitions to be regularly examined,
and salvaged if necessary. 
\par 
Another reason for performing a complete system restart is to commence
execution of a new release of the BOS Server itself. The new-binary restarts
described below do not restart the BOS Server if a new version of its software
has been installed on the machine. 
\par 
\b New-binary \b restarts: New server software may be installed at any time
with the assistance of the BOS Server. However, it is often not the case that
such software installations occur as a result of the discovery of an error in
the program or programs requiring immediate restart. On these occasions,
restarting the given processes in prime time so that the new binaries may begin
execution is counterproductive, causing system downtime and interfering with
user productivity. The system administrator may wish to set an off-peak time
when the server binaries are automatically compared to the running program
images, and restarts take place should the on-disk binary be more recent than
the currently running program. These restarts would thus minimize the resulting
service disruption. 
\par 
Automatically performing these restart functions could be accomplished by
creating cron-type bnodes that were defined to execute at the desired times.
However, rather than force the system administrator to create and supervise
such bnodes, the BOS Server supports the notion of an internal LWP thread with
the same effect (see Section 2.5.2). As part of the BosConfig file defined
above, the administrator may simply specify the values for the complete system
restart and new-binary restart times, and a dedicated BOS Server thread will
manage the restarts. 
\par 
Unless otherwise instructed, the BOS Server selects default times for the two
above restart times. A complete system restart is carried out every Sunday at
4:00am by default, and a new-binary restart is executed for each updated binary
at 5:00am every day. 

	\section sec2-5 Section 2.5: The bosserver Process 

	\subsection sec2-5-1 Section 2.5.1: Introduction 

\par
The user-space bosserver process is in charge of managing the AFS server
processes and software images, the local security and cell database files, and
allowing administrators to execute arbitrary programs on the server machine on
which it runs. It also implements the RPC interface defined in the bosint.xg
Rxgen definition file. 

	\subsection sec2-5-2 Section 2.5.2: Threading 

\par
As with all the other AFS server agents, the BOS Server is a multithreaded
program. It is configured so that a minimum of two lightweight threads are
guaranteed to be allocated to handle incoming RPC calls to the BOS Server, and
a maximum of four threads are commissioned for this task. 
\par
In addition to these threads assigned to RPC duties, there is one other thread
employed by the BOS Server, the BozoDaemon(). This thread is responsible for
keeping track of the two major restart events, namely the system restart and
the new binary restart (see Section 2.4). Every 60 seconds, this thread is
awakened, at which time it checks to see if either deadline has occurred. If
the complete system restart is then due, it invokes internal BOS Server
routines to shut down the entire suite of AFS agents on that machine and then
reexecute the BOS Server binary, which results in the restart of all of the
server processes. If the new-binary time has arrived, the BOS Server shuts down
the bnodes for which binaries newer than those running are available, and then
invokes the new software. 
\par
In general, the following procedure is used when stopping and restarting
processes. First, the restart() operation defined for each bnode's class is
invoked via the BOP RESTART() macro. This allows each server to take any
special steps required before cycling its service. After that function
completes, the standard mechanisms are used to shut down each bnode's process,
wait until it has truly stopped its execution, and then start it back up again. 

	\subsection sec2-5-3 Section 2.5.3: Initialization Algorithm 

\par
This section describes the procedure followed by the BOS Server from the time
when it is invoked to the time it has properly initialized the server machine
upon which it is executing. 
\par
The first check performed by the BOS Server is whether or not it is running as
root. It needs to manipulate local unix files and directories in which only
root has been given access, so it immediately exits with an error message if
this is not the case. The BOS Server's unix working directory is then set to be
/usr/afs/logs in order to more easily service incoming RPC requests to fetch
the contents of the various server log files at this location. Also, changing
the working directory in this fashion results in any core images dumped by the
BOS Server's wards will be left in /usr/afs/logs. 
\par
The command line is then inspected, and the BOS Server determines whether it
will insist on authenticated RPC connections for secure administrative
operations by setting up the /usr/afs/local/NoAuth file appropriately (see
Section 2.3.5). It initializes the underlying bnode package and installs the
three known bnode types (simple, fs, and cron). 
\par
After the bnode package is thus set up, the BOS Server ensures that the set of
local directories on which it will depend are present; refer to Section 2.2 for
more details on this matter. The license file in /usr/afs/etc/License is then
read to determine the number of AFS server machines the site is allowed to
operate, and whether the cell is allowed to run the NFS/AFS Translator
software. This file is typically obtained in the initial system installation,
taken from the installation tape. The BOS Server will exit unless this file
exists and is properly formatted. 
\par
In order to record its actions, any existing /usr/afs/logs/BosLog file is moved
to BosLog.old, and a new version is opened in append mode. The BOS Server
immediately writes a log entry concerning the state of the above set of
important directories. 
\par
At this point, the BOS Server reads the BosConfig file, which lists the set of
bnodes for which it will be responsible. It starts up the processes associated
with the given bnodes. Once accomplished, it invokes its internal system
restart LWP thread (covered in Section 2.5.2 above). 
\par
Rx initialization begins at this point, setting up the RPC infrastructure to
receive its packets on the AFSCONF NANNYPORT, UDP port 7007. The local cell
database is then read and internalized, followed by acquisition of the AFS
encryption keys. 
\par
After all of these steps have been carried out, the BOS Server has gleaned all
of the necessary information from its environemnt and has also started up its
wards. The final initialization action required is to start all of its listener
LWP threads, which are devoted to executing incoming requests for the BOS
Server RPC interface. 

	\subsection sec2-5-4 Section 2.5.4: Command Line Switches 

\par
The BOS Server recognizes exactly one command line argument: -noauth. By
default, the BOS Server attempts to use authenticated RPC connections (unless
the /usr/afs/local/NoAuth file is present; see Section 2.3.5). The appearance
of the -noauth command line flag signals that this server incarnation is to use
unauthenticated connections for even those operations that are normally
restricted to system administrators. This switch is essential during the
initial AFS system installation, where the procedures followed to bootstrap AFS
onto a new machine require the BOS Server to run before some system databases
have been created. 

	\page chap3 Chapter 3: BOS Server Interface 

	\section sec3-1 Section 3.1: Introduction 

\par
This chapter documents the API for the BOS Server facility, as defined by the
bosint.xg Rxgen interface file and the bnode.h include file. Descriptions of
all the constants, structures, macros, and interface functions available to the
application programmer appear in this chapter. 

	\section sec3-2 Section 3.2: Constants 

\par
This section covers the basic constant definitions of interest to the BOS
Server application programmer. These definitions appear in the bosint.h file,
automatically generated from the bosint.xg Rxgen interface file. Another file
is exported to the programmer, namely bnode.h. 
\par
Each subsection is devoted to describing constants falling into each of the
following categories: 
\li Status bits 
\li Bnode activity bits 
\li Bnode states 
\li Pruning server binaries 
\li Flag bits for struct bnode proc 
\par
One constant of general utility is BOZO BSSIZE, which defines the length in
characters of BOS Server character string buffers, including the trailing null.
It is defined to be 256 characters. 

	\subsection sec3-2-1 Section 3.2.1: Status Bits 

\par
The following bit values are used in the flags field of struct bozo status, as
defined in Section 3.3.4. They record whether or not the associated bnode
process currently has a stored core file, whether the bnode execution was
stopped because of an excessive number of errors, and whether the mode bits on
server binary directories are incorrect. 

\par Name
BOZO HASCORE
\par Value
1
\par Description
Does this bnode have a stored core file?

\par Name
BOZO ERRORSTOP
\par Value
2
\par Description
Was this bnode execution shut down because of an excessive number of errors
(more than 10 in a 10 second period)?

\par Name
BOZO BADDIRACCESS
\par Value
3
\par Description
Are the mode bits on the /usr/afs directory and its descendants (etc, bin,
logs, backup, db, local, etc/KeyFile, etc/UserList) correctly set?

	\subsection sec3-2-2 Section 3.2.2: Bnode Activity Bits 

\par
This section describes the legal values for the bit positions within the flags
field of struct bnode, as defined in Section 3.3.8. They specify conditions
related to the basic activity of the bnode and of the entities relying on it. 

\par Name
BNODE NEEDTIMEOUT
\par Value
0x01
\par Description
This bnode is utilizing the timeout mechanism for invoking actions on its
behalf.

\par Name
BNODE ACTIVE
\par Value
0x02
\par Description
The given bnode is in active service.

\par Name
BNODE WAIT
\par Value
0x04
\par Description
Someone is waiting for a status change in this bnode.

\par Name
BNODE DELETE
\par Value
0x08
\par Description
This bnode should be deleted at the earliest convenience.

\par Name
BNODE ERRORSTOP
\par Value
0x10
\par Description
This bnode decommissioned because of an excessive number of errors in its
associated unix processes. 

	\subsection sec3-2-3 Section 3.2.3: Bnode States 

\par
The constants defined in this section are used as values within the goal and
fileGoal fields within a struct bnode. They specify either the current state of
the associated bnode, or the anticipated state. In particular, the fileGoal
field, which is the value stored on disk for the bnode, always represents the
desired state of the bnode, whether or not it properly reflects the current
state. For this reason, only BSTAT SHUTDOWN and BSTAT NORMAL may be used within
the fileGoal field. The goal field may take on any of these values, and
accurately reflects the current status of the bnode. 

\par Name
BSTAT SHUTDOWN
\par Value
0
\par Description
The bnode's execution has been (should be) terminated.

\par Name
BSTAT NORMAL
\par Value
1
\par Description
The bnode is (should be) executing normally.

\par Name
BSTAT SHUTTINGDOWN
\par Value
2
\par Description
The bnode is currently being shutdown; execution has not yet ceased.

\par Name
BSTAT STARTINGUP
\par Value
3
\par Description
The bnode execution is currently being commenced; execution has not yet begun.

	\subsection sec3-2-4 Section 3.2.4: Pruning Server Binaries 

\par
The BOZO Prune() interface function, fully defined in Section 3.6.6.4, allows a
properly-authenticated caller to remove ("prune") old copies of 